---
title: "R Notebook"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

## our goal of classifying and clustring a dataset about liver disease is to facilitate research and analysis about it. since liver disease is more common and can be fatal we are aiming to classify and cluster the patients and analyze the data and correlations between the data.

link to the source of our dataset:
{<https://www.kaggle.com/uciml/indian-liver-patient-records>}

general info about the dataset: it has 11 columns which are: age
(numeric) gender (symmetric binary) Total_Bilirubin (numeric)
direct_Bilirubin (numeric) Alkaline_Phosphotase (numeric)
Alamine_Aminotransferase (numeric) Aspartate_Aminotransferase (numeric)
Total_Protiens (numeric) Albumin (numeric) Albumin_and_Globulin_Ratio
(numeric) Dataset (asymmetric binary)

and has 583 objects and class label (dataset) which indicates (1 disease
or 2 no disease)

## Importing the dataset

read file named "indian_liver_patient" and save it in dataframe named
"dataset"

```{r}
dataset = read.csv("dataset/indian_liver_patient.csv")
```

# Data analysis

raw data and summary of the dataset

```{r}
View(dataset)
str(dataset)
summary(dataset)
```

as shown in the result, we have 4 null values in
Albumin_and_Globulin_Ratio column we can also see there are some values
are quite distant from the 3rd Qu values, in Alamine_Aminotransferase
the max value is 2000 but the 3rd Qu is 60.5, Also in
Alkaline_Phosphotase the max is 2110 but the 3rd Qu is 298 this scenario
is repeating with many attributes which raises the question of whether
or not we have outliers in the Dataset.

the range for the age attribute is quite interesting, since the min
value is 4 and the max value is 90. the age 4 is quite young which is
really concerning considering how dangerous the disease is. we will show
more information about all the attributes through the boxplots.

# Dealing with the missing values

as shown previously we had 4 null values in Albumin_and_Globulin_Ratio
column, as they can bias the results thus, we decided its best to
replace them with the mean value of the column, since deleting those
tuples means we will be missing out on some valuable data.

```{r}
dataset$Albumin_and_Globulin_Ratio = ifelse(is.na(dataset$Albumin_and_Globulin_Ratio), ave(dataset$Albumin_and_Globulin_Ratio, FUN =function(x) mean(x,na.rm=TRUE)), dataset$Albumin_and_Globulin_Ratio)
sum(is.na(dataset))
summary(dataset$Albumin_and_Globulin_Ratio)
```

good, now the null values are replaced with the mean value and to make
sure we checked if the Dataset has any null values and the result is 0
which means there is none.

# Box plot for Age

```{r}
boxplot(dataset$Age, main = "Age", ylab = "Age")
```

the box plot shows no outliers and we can tell that the 1st Qu is at
around 33 and the 3rd Qu is at 58, the range is between 4(min) and
90(max) and the median is at around 45. yet again the young ages are
really concerning, but the median seems quite normal.

# Box plot for Total Bilirubin

```{r}
boxplot(dataset$Total_Bilirubin, main = "Total Bilirubin", ylab = "Total Bilirubin")
```

the boxplot shows many outliers above the max value, lets investigate.
the min value is 0.4 and the max is 75 (range between 0.4 - 75) the 3rd
Qu is at 2.6, there is a big shift between the 3rd Qu and the max, but
many values are between 2.6 and 38 which means these are not true
outliers rather the boxplot showed them to be so, even the value 75 is
quite distant, The range between the 3rd Qu and 38 (where many values
are gathering) is quite big which indicates the range would be large.

# Box plot for Direct Bilirubin

```{r}
boxplot(dataset$Direct_Bilirubin, main = "Direct Bilirubin", ylab = "Direct Bilirubin")
```

Again we could see from the boxplot there are many outliers, so the min
value is 0.1 and the max is 19.7 (range between 0.1 - 19.7)the median is
0.3 and the 3rd Qu is 1.3, we can see a big gap between the 3rd Qu and
the max value but there are many values that are gathering between 1.3
and 12 which indactes these are not true outliers since they are so
redundant this also means the rest of the outliers are not true either
since they are close to 12, again the box plot mistakenly considered
them to be outliers.

# Box plot for Alkaline Phosphotase

```{r}
boxplot(dataset$Alkaline_Phosphotase, main = "Alkaline Phosphotase", ylab = "Alkaline Phosphotase")
```

the boxplot shows many outliers gathering between 500 and 1500, the min
is 63 and the max is 2110 (range between 63 - 2110) and the median is
208 the 3rd Qu is 298, there is a big shift between the 3rd Qu and the
max. yet again since there are many high values it means these are not
true outliers.

# Box plot for Alamine Aminotransferase

```{r}
boxplot(dataset$Alamine_Aminotransferase, main = "Alamine Aminotransferase", ylab = "Alamine Aminotransferase")
```

this boxplot is slightly diffirent since the gathering is concentrated
alomst under 600 but there is a small gathering above them, for the min
value we have 10, max 2000 (range between 10 - 2000) median 35 and 3rd
Qu 60, again we can see a big shift but the range is big hence why the
boxplot considered many values to be outliers.

# Box plot for Aspartate Aminotransferase

```{r}
boxplot(dataset$Aspartate_Aminotransferase, main = "Aspartate Aminotransferase", ylab = "Aspartate Aminotransferase")
```

this boxplot shows the gathering stops at almost 1000 but we have few
values that are reaching 5000, the max is 4929 and the min is 10 (range
between 10 - 4929), the median 42 and the 3rd Qu 87. there is no doubt
that the values under 2000 are not outliers, values higher than that are
also near to them hence we will not be considering them to be outliers.

# Box plot for Total Protiens

```{r}
boxplot(dataset$Total_Protiens, main = "Total Protiens", ylab = "Total Protiens")
```

the boxplot shows some outliers above the max and under the min values,
the min is 2.7 and the max is 9.6 (range between 2.7 - 9.6), the 1st Qu
is 5.8 and the 3rd Qu is 7.2, the median is 6.6, by observing the values
the outliers shown on the boxplot are definitley are not true outliers
since they are so close to each other.

# Box plot for Albumin

```{r}
boxplot(dataset$Albumin, main = "Albumin", ylab="Albumin")
```

the boxplot shows no outliers and the min value is 0.9 the max is 5.5
(range between 0.9 - 5.5) and the median is 3.1 the 3rd Qu is 3.8 the
1st Qu is 2.6 we can see from the values that the range is quite small
compared to other attributes that we went through we will have to deal
with that later.

```{r}
boxplot(dataset$Albumin_and_Globulin_Ratio, main = "Albumin and Globulin Ratio", ylab = "Albumin_and Globulin Ratio")
```

the boxplot is showing the min value to be 0.3 and the max to be 2.8
(range between 0.3 - 2.8), the median to be 0.93 and 3rd Qu of 1.1 and
1st Qu of 0.7, the outliers are shown to start after approximately 1.7
but even the highest outlier is close to the 3rd Qu which means these
are not true outliers.

# Dealing with the Outliers

Although the box plots show some extreme outliers we will not be
removing them. mainly because most of them are not true outliers. also,
According to a doctor those values are possible in liver disease
patients, and each patient tests can vary greatly so we decided its best
to keep those values as it will help the classification procedure.

# Data preprocessing

# encoding cateogrial value (gender)

convert the gender value to numeric to make classification easier, this
step will result in making the male = to 0 and female = to 1.

```{r}
dataset$Gender[dataset$Gender == "Male"] <- 0
dataset$Gender[dataset$Gender == "Female"] <- 1
dataset$Gender=as.numeric(dataset$Gender)
head(dataset)
```

# normalize all numeric attributes

since the range for each attribute is quite different we decided it was
best to normalize the attributes and unify their scale, the new range
for the attributes is from 0 - 1.

```{r}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
dataset$Total_Bilirubin = normalize(dataset$Total_Bilirubin)
dataset$Direct_Bilirubin = normalize(dataset$Direct_Bilirubin)
dataset$Alkaline_Phosphotase = normalize(dataset$Alkaline_Phosphotase)
dataset$Alamine_Aminotransferase = normalize(dataset$Alamine_Aminotransferase)
dataset$Aspartate_Aminotransferase = normalize(dataset$Aspartate_Aminotransferase)
dataset$Total_Protiens = normalize(dataset$Total_Protiens)
dataset$Albumin = normalize(dataset$Albumin)
dataset$Albumin_and_Globulin_Ratio = normalize(dataset$Albumin_and_Globulin_Ratio)
head(dataset)
```

# check correlation

```{r}
cor(dataset$Age,dataset$Dataset)
cor(dataset$Aspartate_Aminotransferase,dataset$Dataset)
cor(dataset$Alkaline_Phosphotase,dataset$Dataset)
cor(dataset$Alamine_Aminotransferase,dataset$Dataset)
cor(dataset$Direct_Bilirubin,dataset$Dataset)
cor(dataset$Total_Bilirubin,dataset$Dataset)
cor(dataset$Total_Protiens,dataset$Dataset)
cor(dataset$Albumin,dataset$Dataset)
cor(dataset$Albumin_and_Globulin_Ratio,dataset$Dataset)

chisq.test(dataset$Gender, dataset$Dataset)
```

we can see there are 4 attributes that are positively correlated to the
class label (Dataset), which are: Gender, Albumin_and_Globulin_Ratio,
Total_Protiens, Albumin. these attributes will be good for splitting the
tree in the classification.

# show dataset after preprocessing

```{r}
head(dataset)
```

# Data visualization

# Create a scatterplot for Alamine_Aminotransferase and Aspartate_Aminotransferase

```{r}
 plot(dataset$Alamine_Aminotransferase, 
     dataset$Aspartate_Aminotransferase, 
     xlab = "Alamine Aminotransferase", 
     ylab = "Aspartate Aminotransferase",
     main = "Scatterplot of Alamine Aminotransferase vs. Aspartate Aminotransferase",
     pch = 16, col = "blue")
```

Alamine_Aminotransferase, Aspartate_Aminotransferase attributes seem to
be correlated (linear relationship), we can see from the plot there is a
major gathering between 0 to 500 on the x-axis and from 0 to 1000 at the
y-axis we can also see that some points are extremely far from the rest
of the data.

# Create a bar plot

```{r}
barplot(table(dataset$Gender),
        ylab = "Frequency",
        xlab = "Gender",
        col =" blue")
```

we can see from the plot that female patients are less than half the
number of male patients which means that the dataset is unbalanced.

# carete a scatter plot for Age and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Age, 
     xlab = "dataset", 
     ylab = "Age",
     main = "Age vs. dataset",
     pch = 16, col = "blue")
 
```

from the plot we can see the same age groups can be found in both
patients with liver disease(1) and patients without liver disease(2),
which indicates the variety in ages. what is concerning though is many
young kids are diagnosed with liver disease, we thought its highly
unlikely for kids to be diagnosed with such diseases but as it turns out
this is not the case, in fact less older patients are diagnosed with
liver disease.

# carete a scatter plot for Total_Bilirubin and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Total_Bilirubin, 
     xlab = "dataset", 
     ylab = "Total_Bilirubin",
     main = "Total_Bilirubin vs. dataset",
     pch = 16, col = "blue")
```

from the graph it seems the higher the Total_Bilirubin the more liklely
the patient is diagnosed with liver disease(1).

# carete a scatter plot for Direct_Bilirubin and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Direct_Bilirubin, 
     xlab = "dataset", 
     ylab = "Direct_Bilirubin",
     main = "Direct_Bilirubin vs. dataset",
     pch = 16, col = "blue")
```

Again, from the graph it seems the higher the Direct_Bilirubin the more
liklely the patient is diagnosed with liver disease(1).

# carete a scatter plot for Alkaline_Phosphotase and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Alkaline_Phosphotase, 
     xlab = "dataset", 
     ylab = "Alkaline_Phosphtase",
     main = "Alkaline_Phosphtase vs. dataset",
     pch = 16, col = "blue")
```

the plot shows that the patients diagnosed with liver disease(1)
experience high levels of Alkaline_Phosphotase, it also show one point
where the patient is not diagnosed with liver disease(2) but experience
a relatively high level of Alkaline_Phosphotase.

# carete a scatter plot for Alamine_Aminotransferase and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Alamine_Aminotransferase, 
     xlab = "dataset", 
     ylab = "Alamine_Aminotransferase",
     main = "Alamine_Aminotransferase vs. dataset",
     pch = 16, col = "blue")
```

in this plot we can see that patients not diagnosed with liver
disease(2) have low levels of Alamine_Aminotransferase its range is
between 0 ans 0.1, while patients diagnosed with liver disease have
rally high levels of Alamine_Aminotransferase.

# carete a scatter plot for Aspartate_Aminotransferase and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Aspartate_Aminotransferase, 
     xlab = "dataset", 
     ylab = "Aspartate_Aminotransferase",
     main = "Aspartate_Aminotransferase vs. dataset",
     pch = 16, col = "blue")
```

as we can see from the plot, most paitents diagnosed with liver
disease(1) have levels lower than 0.4 and few of them have values
greater than 0.4, this could indicate the more of this enzyme means
there might problems with the liver. patients not diagnosed with liver
disease(2) have levels less than 0.1 .

# carete a scatter plot for Total_Protiens and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Total_Protiens, 
     xlab = "dataset", 
     ylab = "Total_Protiens",
     main = "Total_Protiens vs. dataset",
     pch = 16, col = "blue")
```

this is quite intersting, form the plot both patients with and without
liver disease have almost the same levels of Total_Protiens, but we can
see that patients with liver disease (1) have values less than 0.2 and
values higher than 0.8 which is not present in patients without liver
disease(2).

# carete a scatter plot for Albumin and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Albumin, 
     xlab = "dataset", 
     ylab = "Albumin",
     main = "Albumin vs. dataset",
     pch = 16, col = "blue")
```

again we see both patients with and without liver disease have almost
the same levels of Albumin, but yet again we see some values with the
patients diagnosed with liver disease(1) that are less than 0.1 and more
than 0.9 which is not present with patients not diagnosed with liver
disease(2).

# carete a scatter plot for Albumin_and_Globulin_Ratio and class label (dataset)

```{r}
plot(dataset$Dataset, 
     dataset$Albumin, 
     xlab = "dataset", 
     ylab = "Albumin_and_Globulin_Ratio",
     main = "Albumin_and_Globulin_Ratio vs. dataset",
     pch = 16, col = "blue")
```

again we see both patients with and without liver disease have almost
the same levels of Albumin, but yet again we see some values with the
patients diagnosed with liver disease(1) that are less than 0.1 and more
than 0.9 which is not present with patients not diagnosed with liver
disease(2).

# create histogram to see the number of patients with liver disease

```{r}
hist(dataset$Dataset,
     col  = "blue", 
     xlab = "Patients",    
     ylab = "Frequency",      
     main = "patient with liver disease or no disease")
```

we can see from the plot that patients diagnosed with liver disease are
almost 2x patients not diagnosed with liver disease in the dataset
which, again indecates that the dataset is unbalanced we will have to
pay attention to that when it comes to the classification procedure.


```{r}
prdataset = read.csv("dataset/proccesseddataset.csv")
```

# classification

we will be using random sampling.

```{r}
library(RWeka)
library(caret)
library(e1071)
library(C50)
library(boot)
library(caTools)
library(rpart)
library(rpart.plot)
```

```{r}
# Set the seed for reproducibility
set.seed(123)
```

# size for training 70% using random sampling

```{r}
# Split the dataset into training and testing sets
split <- sample.split(prdataset$Dataset, SplitRatio = 0.7)
```

```{r}
# Create the training set
training_set <- prdataset[split, ]

# Create the testing set
testing_set <- prdataset[!split, ]
```

```{r}
# Build the decision tree using information gain
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class", parms = list(split = "information"))
```

```{r}
# Plot the decision tree
rpart.plot(decision_tree)
```

## Make predictions on the testing set

```{r}
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

## Compare the predictions with the actual values

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

decision tree model based on the confusion matrix has an accuracy of
0.6686, indicating that it correctly predicted the outcome in 66.86% of
cases. It shows fair agreement with a kappa value of 0.0514 The model
has higher sensitivity (0.8640) for identifying positive cases and lower
specificity (0.1800) for identifying negative cases. The positive
predictive value (precision) is 0.7248, and the negative predictive
value is 0.3462 The prevalence of the positive class is 0.7143, and the
model's detection rate is 0.6171 The balanced accuracy, considering
both sensitivity and specificity, is 0.5220.


## Build the decision tree using gain ratio

```{r}
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class", parms = list(split = "gain"))
```

## Plot the decision tree

```{r}
rpart.plot(decision_tree)
```

## Make predictions on the testing set

```{r}
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

## Compare the predictions with the actual values

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model achieved an accuracy of 0.6857, slightly better than the no
information rate. The kappa coefficient indicates fair agreement with
actual classes. Sensitivity is high for positive cases, but specificity
is low for negative cases. Precision is 0.7397 for positive cases and
0.4138 for negative cases. The model's detection rate is 0.6171, and its
detection prevalence is 0.8343. The balanced accuracy is 0.5520.

Overall, the model shows good sensitivity for positive cases(disease =
1) but lower specificity for negative cases(disease = 2). 

# Build the decision tree using the Gini index

```{r}
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class")
```

# Plot the decision tree

```{r}
rpart.plot(decision_tree)
```

# Make predictions on the testing set

```{r}
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

# Compare the predictions with the actual values

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model achieved an accuracy of 0.6857, slightly better than the no
information rate. The kappa coefficient indicates fair agreement with
actual classes. Sensitivity is high for positive cases, but specificity
is low for negative cases. Precision is 0.7397 for positive cases and
0.4138 for negative cases. The model's detection rate is 0.6171, and its
detection prevalence is 0.8343. The balanced accuracy is 0.5520.

Overall, the model shows good sensitivity for positive cases(disease =
1) but lower specificity for negative cases(disease = 2).

## What is the best tree for this size ?

all three trees have similar accuracy and balanced accuracy. However,
the first tree (Information Gain) has a lower sensitivity and accuracy
compared to the other two trees. Sensitivity measures the proportion of
true positive cases correctly identified by the model, which is an
important metric. therefore the gini index and the gain ratio are better.

# size for training 60%:

```{r}
#Set the seed for reproducibility 60%
set.seed(123)
```

## Split the dataset into training and testing sets

```{r}
split <- sample.split(prdataset$Dataset, SplitRatio = 0.6)
```

## Create the training set

```{r}
training_set <- prdataset[split, ]
```

## Create the testing set

```{r}
testing_set <- prdataset[!split, ]
```

## Build the decision tree using information gain

```{r}
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class", parms = list(split = "information"))
```

## Plot the decision tree

```{r}
rpart.plot(decision_tree)
```

```{r}
# Make predictions on the testing set
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The provided statistics describe the performance of a binary
classification model. The model's accuracy is 0.7039, meaning it
correctly predicts the class of 70.39% of instances. the
sensitivity (0.8494) and specificity (0.8494) . The positive predictive
value (precision) is 0.7622, indicating that when the model predicts a
positive instance, it is correct 76.22% of the time. The negative
predictive value is 0.4792, representing the proportion of correctly
predicted negative instances out of all predicted negatives. The
prevalence of class 1 in the dataset is 71.24%, and the balanced
accuracy, which is the average of sensitivity and specificity, is
59.63%.

## Build the decision tree using gain ratio

```{r}
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class", parms = list(split = "gain"))
```

```{r}
# Plot the decision tree
rpart.plot(decision_tree)
```

```{r}
# Make predictions on the testing set
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model has an accuracy of 0.7253 and shows a fair agreement beyond
chance with a kappa value of 0.2984 It has a higher sensitivity
(0.8373) for correctly identifying positive instances but a lower
specificity (0.4478) for correctly identifying negative instances. The
positive predictive value is 0.7898 and the negative predictive value is
0.5263.

## Build the decision tree using the Gini Index

```{r}
# Build the decision tree using the Gini index
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class")
```

```{r}
# Plot the decision tree
rpart.plot(decision_tree)
```

```{r}
# Make predictions on the testing set
predictions <- predict(decision_tree, newdata = testing_set, type = "class")

```

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model has an accuracy of 0.7253 and shows a fair agreement beyond
chance with a kappa value of 0.2984 It has a higher sensitivity
(0.8373) for correctly identifying positive instances but a lower
specificity (0.4478) for correctly identifying negative instances. The
positive predictive value is 0.7898 and the negative predictive value is
0.5263.

## What is the best tree for this size?

Based on the results from Confusion Matrix and Statistics, it appears
that the Gain Ratio and Gini Index methods perform similarly, but
slightly better than the Information Gain method.

# size for training 80%

```{r}
# Set the seed for reproducibility
set.seed(123)
```

## Split the dataset into training and testing sets

```{r}

split <- sample.split(prdataset$Dataset, SplitRatio = 0.8)
```

```{r}
# Create the training set
training_set <- prdataset[split, ]

# Create the testing set
testing_set <- prdataset[!split, ]
```

```{r}
# Build the decision tree using information gain
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class", parms = list(split = "information"))
```

```{r}
# Plot the decision tree
rpart.plot(decision_tree)
```

```{r}
# Make predictions on the testing set
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model achieved an accuracy of 0.7069, correctly predicting the class
labels for 70.69% of the instances. It demonstrated a high sensitivity of
0.8795, indicating that it accurately identified a large proportion of
the positive instances. However, the specificity was low at 27.27%,
indicating that it struggled to accurately identify negative instances.
The positive predictive value -precision- was 0.7526, meaning that a
significant portion of instances predicted as positive were indeed
positive. The negative predictive value was 0.4737, indicating that a
moderate proportion of instances predicted as negative were actually
negative. The prevalence of the positive class was approximately 71.55%.
The balanced accuracy, which considers both sensitivity and specificity,
was 0.5761.

Overall, the model demonstrated good performance in identifying positive
instances but had limitations in accurately identifying negative
instances.

```{r}
# Build the decision tree using gain ratio
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class", parms = list(split = "gain"))
```

```{r}
# Plot the decision tree
rpart.plot(decision_tree)
```

```{r}
# Make predictions on the testing set
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model achieved an accuracy of 0.7241, indicating that it correctly
predicted the class labels for 72.41% of the instances. It demonstrated a
high sensitivity of 91.57%, meaning it accurately identified a large
proportion of the positive instances. However, the specificity was low
at 24.24%, indicating that it struggled to accurately identify negative
instances. The positive predictive value (precision) was 75.25%,
indicating that a significant portion of instances predicted as positive
were indeed positive. The negative predictive value was 53.33%

The model performs well in identifying positive instances but struggles
with negative instances. Positive predictive value is decent, but
negative predictive value is low. Overall, the model's performance is
moderate.

```{r}
# Build the decision tree using the Gini index
decision_tree <- rpart(Dataset ~ ., data = training_set, method = "class")
```

```{r}
# Plot the decision tree
rpart.plot(decision_tree)
```

```{r}
# Make predictions on the testing set
predictions <- predict(decision_tree, newdata = testing_set, type = "class")
```

```{r}
table(predictions, testing_set$Dataset)
predictions <- factor(predictions, levels = c("1", "2"))
testing_set$Dataset <- factor(testing_set$Dataset, levels = c("1", "2"))

confusionMatrix(predictions, testing_set$Dataset)
```

The model achieved an accuracy of 0.7241, indicating that it correctly
predicted the class labels for 72.41% of the instances. It demonstrated a
high sensitivity of 91.57%, meaning it accurately identified a large
proportion of the positive instances. However, the specificity was low
at 24.24%, indicating that it struggled to accurately identify negative
instances. The positive predictive value (precision) was 75.25%,
indicating that a significant portion of instances predicted as positive
were indeed positive. The negative predictive value was 53.33%

The model performs well in identifying positive instances but struggles
with negative instances. Positive predictive value is decent, but
negative predictive value is low. Overall, the model's performance is
moderate.

## What is the best tree for this size?

Based on the results from Confusion Matrix and Statistics, the best tree
would be the one using the Gain ratio or Gini index .

Although all trees have the same accuracy of 72.41%, the Gain ratio tree
and Gini index tree has a higher sensitivity of 91.57% compared to the
information gain tree (87.95%). This indicates that the Gini index
tree,and Gain ratio performs better in correctly identifying positive
instances. Additionally, the Gini index tree and Gain ratio tree has a
slightly higher balanced accuracy of 0.5790 compared to 0.5761 for the
information gain tree.

Therefore, considering the higher sensitivity and slightly higher
balanced accuracy, the Gini index tree and Gain ratio appears to be the
better choice based on the provided results.

# What is the best size for training data?

1.  Size for training 70% (gini index and gain ratio):

    -   Accuracy: 68.57%

    -   Kappa: 0.1119

    -   Sensitivity: 86.4%

    -   Specificity: 24%

    -   Balanced Accuracy: 55.2%

    -   Positive Predictive Value: 73.97%

    -   Negative Predictive Value: 41.38%

2.  Size for training 60% (gini index and gain ratio):

    -   Accuracy: 72.53%

    -   Kappa: 0.2984

    -   Sensitivity: 83.73%

    -   Specificity: 44.78%

    -   Balanced Accuracy: 64.26%

    -   Positive Predictive Value: 78.98%

    -   Negative Predictive Value: 52.63%

3.  Size for training 80% (gini index and gain ratio):

    -   Accuracy: 72.41%

    -   Kappa: 0.1892

    -   Sensitivity: 91.57%

    -   Specificity: 24.24%

    -   Balanced Accuracy: 57.9%

    -   Positive Predictive Value: 75.25%

    -   Negative Predictive Value: 53.33%

considiring that our dataset is imbalanced we have to note that accuracy is not our bestfriend here, rather sensitivity and Specificity are, we will carry our desicion based on that.

Based on these results, the model trained on 60% of the data performs
the best, with the highest accuracy, kappa value, Specificity, and
balanced accuracy.


#clustring
#our goal of clustering is to enhance our understanding of liver disease by grouping patients with similar attributes and to uncover hidden patterns or groupings within the patient data that may not be immediately apparent to us.
```{r}
#--define function --
# Function to calculate BCubed Precision
calculate_bcubed_precision <- function(data) {
  precision_values <- numeric(nrow(data))
  
  for (i in 1:nrow(data)) {
    # Get all points in the same cluster
    same_cluster <- data$Cluster[i] == data$Cluster
    
    # Get all points with the same external label
    same_label <- data$Label[i] == data$Label
    
    # Calculate precision for this point
    precision_values[i] <- sum(same_cluster & same_label) / sum(same_cluster)
  }
  
 
  mean(precision_values)
}

# Function to calculate BCubed Recall
calculate_bcubed_recall <- function(data) {
  recall_values <- numeric(nrow(data))
  
  for (i in 1:nrow(data)) {
    # Get all points with the same external label
    same_label <- data$Label[i] == data$Label
    
    # Get all points in the same cluster as the current point
    same_cluster <- data$Cluster[i] == data$Cluster
    
    # Calculate recall for this point
    recall_values[i] <- sum(same_cluster & same_label) / sum(same_label)
  }
  
  
  mean(recall_values)
}

external_labels <- prdataset$Dataset  # To return it in bcubed  
dataset =subset (prdataset, select = -c(Dataset))#Since clustering is an unsupervised learning method, we should omit the class labels.

```


#------------k=2-------------   

```{r}
k <- 2
set.seed(8953)
prdataset=scale(prdataset)
kmeans.result <- kmeans(prdataset, 2)
# clusterng result
kmeans.result

#-----Average Silhouette width ----
library(fpc)
cluster_assignments <- kmeans.result$cluster
diss_matrix <- dist(prdataset)
silhouette_avg <- cluster.stats(diss_matrix, cluster_assignments)$avg.silwidth
cat("Average Silhouette Width:", silhouette_avg, "\n")

#-----calculate total within-cluster sum of square----
total_wss_k2 <- kmeans.result$tot.withinss
print(paste("Total within-cluster sum of squares for k=2: ", total_wss_k2))

#-----calculate bcubed---
combined_data <- data.frame(Cluster = kmeans.result$cluster, Label = external_labels)
bcubed_precision <- calculate_bcubed_precision(combined_data)
bcubed_recall <- calculate_bcubed_recall(combined_data)

print(paste("BCubed Precision for k =", k, ":", bcubed_precision))
print(paste("BCubed Recall for k =", k, ":", bcubed_recall))

# visualize clustering
library(factoextra)
fviz_cluster(kmeans.result, data = prdataset)


```

#------------k=3------------- 

```{r}
k <- 3
set.seed(8953)
prdataset=scale(prdataset)
kmeans.result <- kmeans(prdataset, 3)
# clusterng result
kmeans.result

#-----Average Silhouette width ----
library(fpc)
cluster_assignments <- kmeans.result$cluster
diss_matrix <- dist(prdataset)
silhouette_avg <- cluster.stats(diss_matrix, cluster_assignments)$avg.silwidth
cat("Average Silhouette Width:", silhouette_avg, "\n")

#-----calculate total within-cluster sum of square----
total_wss_k3 <- kmeans.result$tot.withinss
print(paste("Total within-cluster sum of squares for k=3: ", total_wss_k3))

#-----calculate bcubed---
combined_data <- data.frame(Cluster = kmeans.result$cluster, Label = external_labels)
bcubed_precision <- calculate_bcubed_precision(combined_data)
bcubed_recall <- calculate_bcubed_recall(combined_data)

print(paste("BCubed Precision for k =", k, ":", bcubed_precision))
print(paste("BCubed Recall for k =", k, ":", bcubed_recall))

# visualize clustering
library(factoextra)
fviz_cluster(kmeans.result, data = prdataset)


```

#------------k=4-------------
```{r}

k <- 4
set.seed(8953)
prdataset=scale(prdataset)
kmeans.result <- kmeans(prdataset, 4)
# clusterng result
kmeans.result

#-----Average Silhouette width ----
library(fpc)
cluster_assignments <- kmeans.result$cluster
diss_matrix <- dist(prdataset)
silhouette_avg <- cluster.stats(diss_matrix, cluster_assignments)$avg.silwidth
cat("Average Silhouette Width:", silhouette_avg, "\n")

#-----calculate total within-cluster sum of square----
total_wss_k4 <- kmeans.result$tot.withinss
print(paste("Total within-cluster sum of squares for k=4: ", total_wss_k4))

#-----calculate bcubed---
combined_data <- data.frame(Cluster = kmeans.result$cluster, Label = external_labels)
bcubed_precision <- calculate_bcubed_precision(combined_data)
bcubed_recall <- calculate_bcubed_recall(combined_data)

print(paste("BCubed Precision for k =", k, ":", bcubed_precision))
print(paste("BCubed Recall for k =", k, ":", bcubed_recall))

# visualize clustering
library(factoextra)
fviz_cluster(kmeans.result, data = prdataset)

```

# Find optimal number of clusters
```{r}
set.seed(123)  # Set seed for reproducibility
wss <- numeric(10) 

  for (k in 1:10) {
  # Run the k-means algorithm on the dataset
  M <- kmeans(prdataset, centers = k, nstart = 25)
  # Store the within-cluster sum of squares
  wss[k] <- M$tot.withinss
    }

# We use the elbow method to find the optimal number of clusters.
plot(1:10, wss, type = "b", xlab = "Number of Clusters", ylab = "WCSS",
     main = "Elbow Method for Optimal Number of Clusters")

```


#compare the result of clustring 
The optimal number of clusters for the data presented appears to be 2, based on the given metrics. Although the Average Silhouette Width is highest for k=4 (0.2251056), indicating better cluster tightness and separation, the decision for the optimal k should also consider the Elbow Method, BCubed Precision, and BCubed Recall. The Elbow Method graph shows a notable angle at k=2, suggesting diminishing returns in variance reduction for higher values of k. Furthermore, k=2 has the highest BCubed Precision (0.618258183992743) indicating the highest accuracy of cluster assignments. The recall for k=2 is also relatively high (0.602630683348964) compared to k=3 and k=4. While there is a trade-off between precision and recall, the combined evidence from the Elbow Method and BCubed metrics supports k=2 as the optimal choice for balancing cluster quality and quantity.
#why we choose size of k /
k=2: Offers the clearest grouping with high similarity within clusters and the most distinct separation between them, as indicated by the highest Average Silhouette Width and BCubed Precision.

k=3: Introduces a more nuanced division of the data which could reveal additional patterns, despite a decrease in both silhouette score and BCubed Recall, suggesting less distinct clustering.

k=4: Provides even finer granularity, potentially overfitting, with the lowest within-cluster sum of squares, but the cluster distinction and data point assignment accuracy are not as strong as with fewer clusters.





